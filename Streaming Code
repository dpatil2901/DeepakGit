from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_extract
from pyspark.sql.types import *

def create_spark_session():
    spark = SparkSession.builder \
        .appName('Read DF').master('local[*]') \
        .getOrCreate()
    return  spark

mySchema=StructType([StructField('EventLogs',StringType())])

def read_log():
    logDf = spark.readStream.format('csv')\
        .schema(mySchema).load('hdfs://cdhserver:8020/user/labuser/streaming')
    return logDf

if __name__ == '__main__':
    spark=create_spark_session()
    eventlogDf=read_log()
    # eventlogDf.show(truncate=False)
    ip_add=r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'
    dt_col=r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}-\d{4)}\]'
    url_col=r'"([^"]*)"'
    redDf=eventlogDf\
        .withColumn('ip_address',regexp_extract('EventLogs',ip_add,0)) \
        .withColumn('dt_col', regexp_extract('EventLogs', dt_col, 1)) \
        .withColumn('url_col', regexp_extract('EventLogs', url_col, 1)) \
        .select('ip_address')
    #redDf.show(truncate=False)

    result = redDf.writeStream.format('console').outputMode('append').start()
    result.awaitTermination()
